{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently training on: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check for CUDA!\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Currently training on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "###########                      PARAMETERS                   ##################\n",
    "################################################################################\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "WINDOW_SIZE = 255\n",
    "\n",
    "# Trainning parameters\n",
    "DROPOUT = 0.00\n",
    "CLIP = -1\n",
    "N_EPOCHS = 10\n",
    "LEARNING_RATE = 3e-4\n",
    "\n",
    "# Network parameters\n",
    "INPUT_SIZE = 1\n",
    "OUTPUT_SIZE = 1\n",
    "CHANNEL_SIZES = [32] * 4\n",
    "KERNEL_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "###########                   Import Dataset(s)               ##################\n",
    "################################################################################\n",
    "from os import walk, path\n",
    "from data.file_handler import filter_filenames, read_file\n",
    "from data.processing import parse_str\n",
    "from config import BASE, DATASET_DIR, DATA_REG, LABEL_REG\n",
    "from torch.utils.data import DataLoader\n",
    "from data.dataset import CustomDataset\n",
    "\n",
    "# Get paths\n",
    "filenames = next(walk(path.join(path.abspath('../'), \"dataset\")), (None, None, []))[2]  # extract all files from dataset folder\n",
    "# file_paths = [\"./dataset/data64QAM.txt\", \"./dataset/OSC_sync_291.txt\", \"./dataset/OSC_sync_292.txt\", \"./dataset/OSC_sync_293.txt\"]\n",
    "input_filenames = filter_filenames(filenames, DATA_REG) # filter the files, we only want output signals\n",
    "input_filenames.sort()\n",
    "label_filenames = filter_filenames(filenames, LABEL_REG)\n",
    "label_filenames.sort()\n",
    "\n",
    "array_data = []\n",
    "array_labels = []\n",
    "\n",
    "# We are specifally training on \"OSC_sync_471.txt\"\n",
    "if input_filenames.count(\"OSC_sync_471.txt\"):\n",
    "  array_data = parse_str(\n",
    "    read_file(path.join(path.abspath('../'), DATASET_DIR, \"OSC_sync_471.txt\"))\n",
    "  )\n",
    "else:\n",
    "  print(\"OSC_sync_471.txt not found!\")\n",
    "\n",
    "if label_filenames.count(\"data64QAM.txt\"):\n",
    "  array_labels = parse_str(\n",
    "    read_file(path.join(path.abspath('../'), DATASET_DIR, \"data64QAM.txt\")\n",
    "    )\n",
    "  )\n",
    "else:\n",
    "  print(\"data64QAM.txt not found!\")\n",
    "\n",
    "dataset = CustomDataset(\"../dataset\", \"OSC_sync_471.txt\",\"data64QAM.txt\", 255)\n",
    "\n",
    "# Split into train and validation sets\n",
    "train_size = int(0.6 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# dataset = CustomDataset_1D(array_data, array_labels, 255, train=True, train_ratio=0.6)\n",
    "# dataloader = DataLoader(dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "# validation_dataset = CustomDataset_1D(\n",
    "#   array_data, array_labels, 255, train=False, train_ratio=0.6\n",
    "# )\n",
    "# validation_dataloader = DataLoader(\n",
    "#   validation_dataset, batch_size=512\n",
    "# )\n",
    "\n",
    "# train_count = 0\n",
    "# flag = False\n",
    "# print(f\"Training dataloader iterations: {len(dataloader)}\")\n",
    "# print(f\"Validation dataloader iterations: {len(validation_dataloader)}\")\n",
    "# for idx, data in enumerate(dataloader):\n",
    "#     datas = data[0]\n",
    "#     labels = data[1]\n",
    "#     if not flag:\n",
    "#       print(\"Data shape:\", datas.shape)\n",
    "#       print(\"Label shape:\", labels.shape)\n",
    "#       flag = True\n",
    "#     train_count += datas.shape[0]\n",
    "#     # break\n",
    "# print(f\"Training dataset count: {train_count}\")\n",
    "# test_count = 0\n",
    "# flag = False\n",
    "# for idx, data in enumerate(validation_dataloader):\n",
    "#     datas = data[0]\n",
    "#     labels = data[1]\n",
    "#     if not flag:\n",
    "#       print(\"Data shape:\", datas.shape)\n",
    "#       print(\"Label shape:\", labels.shape)\n",
    "#       flag = True\n",
    "#     test_count += datas.shape[0]\n",
    "#     # break\n",
    "# print(f\"Validation dataset count: {test_count}\")\n",
    "# print(f\"Total count: {train_count + test_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\dsp-project\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiTCN(\n",
      "  (tcn): TemporalConvNet(\n",
      "    (network): Sequential(\n",
      "      (0): TemporalBlock(\n",
      "        (conv1): Conv1d(1, 32, kernel_size=(16,), stride=(1,), padding=(15,))\n",
      "        (chomp1): Chomp1D()\n",
      "        (relu1): PReLU(num_parameters=1)\n",
      "        (dropout1): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): Conv1d(32, 32, kernel_size=(16,), stride=(1,), padding=(15,))\n",
      "        (chomp2): Chomp1D()\n",
      "        (relu2): PReLU(num_parameters=1)\n",
      "        (dropout2): Dropout(p=0.0, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(1, 32, kernel_size=(16,), stride=(1,), padding=(15,))\n",
      "          (1): Chomp1D()\n",
      "          (2): PReLU(num_parameters=1)\n",
      "          (3): Dropout(p=0.0, inplace=False)\n",
      "          (4): Conv1d(32, 32, kernel_size=(16,), stride=(1,), padding=(15,))\n",
      "          (5): Chomp1D()\n",
      "          (6): PReLU(num_parameters=1)\n",
      "          (7): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (downsample): Conv1d(1, 32, kernel_size=(1,), stride=(1,))\n",
      "        (relu): PReLU(num_parameters=1)\n",
      "      )\n",
      "      (1): TemporalBlock(\n",
      "        (conv1): Conv1d(32, 32, kernel_size=(16,), stride=(1,), padding=(30,), dilation=(2,))\n",
      "        (chomp1): Chomp1D()\n",
      "        (relu1): PReLU(num_parameters=1)\n",
      "        (dropout1): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): Conv1d(32, 32, kernel_size=(16,), stride=(1,), padding=(30,), dilation=(2,))\n",
      "        (chomp2): Chomp1D()\n",
      "        (relu2): PReLU(num_parameters=1)\n",
      "        (dropout2): Dropout(p=0.0, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(32, 32, kernel_size=(16,), stride=(1,), padding=(30,), dilation=(2,))\n",
      "          (1): Chomp1D()\n",
      "          (2): PReLU(num_parameters=1)\n",
      "          (3): Dropout(p=0.0, inplace=False)\n",
      "          (4): Conv1d(32, 32, kernel_size=(16,), stride=(1,), padding=(30,), dilation=(2,))\n",
      "          (5): Chomp1D()\n",
      "          (6): PReLU(num_parameters=1)\n",
      "          (7): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (relu): PReLU(num_parameters=1)\n",
      "      )\n",
      "      (2): TemporalBlock(\n",
      "        (conv1): Conv1d(32, 32, kernel_size=(16,), stride=(1,), padding=(60,), dilation=(4,))\n",
      "        (chomp1): Chomp1D()\n",
      "        (relu1): PReLU(num_parameters=1)\n",
      "        (dropout1): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): Conv1d(32, 32, kernel_size=(16,), stride=(1,), padding=(60,), dilation=(4,))\n",
      "        (chomp2): Chomp1D()\n",
      "        (relu2): PReLU(num_parameters=1)\n",
      "        (dropout2): Dropout(p=0.0, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(32, 32, kernel_size=(16,), stride=(1,), padding=(60,), dilation=(4,))\n",
      "          (1): Chomp1D()\n",
      "          (2): PReLU(num_parameters=1)\n",
      "          (3): Dropout(p=0.0, inplace=False)\n",
      "          (4): Conv1d(32, 32, kernel_size=(16,), stride=(1,), padding=(60,), dilation=(4,))\n",
      "          (5): Chomp1D()\n",
      "          (6): PReLU(num_parameters=1)\n",
      "          (7): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (relu): PReLU(num_parameters=1)\n",
      "      )\n",
      "      (3): TemporalBlock(\n",
      "        (conv1): Conv1d(32, 32, kernel_size=(16,), stride=(1,), padding=(120,), dilation=(8,))\n",
      "        (chomp1): Chomp1D()\n",
      "        (relu1): PReLU(num_parameters=1)\n",
      "        (dropout1): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): Conv1d(32, 32, kernel_size=(16,), stride=(1,), padding=(120,), dilation=(8,))\n",
      "        (chomp2): Chomp1D()\n",
      "        (relu2): PReLU(num_parameters=1)\n",
      "        (dropout2): Dropout(p=0.0, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(32, 32, kernel_size=(16,), stride=(1,), padding=(120,), dilation=(8,))\n",
      "          (1): Chomp1D()\n",
      "          (2): PReLU(num_parameters=1)\n",
      "          (3): Dropout(p=0.0, inplace=False)\n",
      "          (4): Conv1d(32, 32, kernel_size=(16,), stride=(1,), padding=(120,), dilation=(8,))\n",
      "          (5): Chomp1D()\n",
      "          (6): PReLU(num_parameters=1)\n",
      "          (7): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (relu): PReLU(num_parameters=1)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=16320, out_features=1, bias=True)\n",
      ")\n",
      "Starting epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 0.4683, Validation Loss: 0.0366\n",
      "Saved model with validation loss: 0.0366\n",
      "Starting epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Train Loss: 0.0248, Validation Loss: 0.0211\n",
      "Saved model with validation loss: 0.0211\n",
      "Starting epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Train Loss: 0.0173, Validation Loss: 0.0154\n",
      "Saved model with validation loss: 0.0154\n",
      "Starting epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Train Loss: 0.0141, Validation Loss: 0.0142\n",
      "Saved model with validation loss: 0.0142\n",
      "Starting epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Train Loss: 0.0137, Validation Loss: 0.0139\n",
      "Saved model with validation loss: 0.0139\n",
      "Starting epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Train Loss: 0.0134, Validation Loss: 0.0136\n",
      "Saved model with validation loss: 0.0136\n",
      "Starting epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Train Loss: 0.0132, Validation Loss: 0.0135\n",
      "Saved model with validation loss: 0.0135\n",
      "Starting epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_EPOCHS):\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mN_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 59\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train(model, device, train_loader, optimizer, criterion)\n\u001b[0;32m     60\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m validate(model, device, val_loader, criterion)\n\u001b[0;32m     61\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[1;32mIn[6], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, device, train_loader, optimizer, criterion)\u001b[0m\n\u001b[0;32m     27\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     28\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     30\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     31\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(X_batch\u001b[38;5;241m.\u001b[39mto(device))\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dsp-project\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dsp-project\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:697\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 697\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_name):\n\u001b[0;32m    698\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m             \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dsp-project\\Lib\\site-packages\\torch\\autograd\\profiler.py:738\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[1;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_enter_new(\n\u001b[0;32m    734\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\n\u001b[0;32m    735\u001b[0m     )\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m--> 738\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type: Any, exc_value: Any, traceback: Any):\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_callbacks_on_exit:\n\u001b[0;32m    740\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nn.TCNN1 import BiTCN\n",
    "\n",
    "model = BiTCN(\n",
    "  INPUT_SIZE,\n",
    "  OUTPUT_SIZE,\n",
    "  CHANNEL_SIZES,\n",
    "  KERNEL_SIZE,\n",
    "  seq_len=WINDOW_SIZE,\n",
    "  dropout=DROPOUT\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "  model.parameters(),\n",
    "  lr = LEARNING_RATE\n",
    ")\n",
    "scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "def train(model, device,train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch.to(device))\n",
    "        loss = criterion(output, y_batch.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "    return average_loss\n",
    "\n",
    "def validate(model, device, val_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
    "            output = model(X_batch.to(device))\n",
    "            loss = criterion(output, y_batch.to(device))\n",
    "            total_loss += loss.item()\n",
    "        average_loss = total_loss / len(val_loader)\n",
    "    return average_loss\n",
    "\n",
    "# Train the model\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f\"Starting epoch {epoch + 1}/{N_EPOCHS}\")\n",
    "\n",
    "    train_loss = train(model, device, train_loader, optimizer, criterion)\n",
    "    val_loss = validate(model, device, val_loader, criterion)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{N_EPOCHS}], Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        # torch.save(model.state_dict(), 'BiTCN_best_model.pth')\n",
    "        print(f\"Saved model with validation loss: {best_val_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsp-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
