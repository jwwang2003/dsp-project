{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Classification Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"JUN WEI WANG\"\n",
    "__email__ = \"wjw_03@outlook.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently training on: cuda\n",
      "Your current working directory C:\\Users\\wjw_0\\dsp-project\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# from IPython.display import set_matplotlib_formats\n",
    "# %matplotlib inline\n",
    "# set_matplotlib_formats('svg')\n",
    "\n",
    "# Check for CUDA!\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Currently training on: {device}\")\n",
    "\n",
    "print(\"Your current working directory\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODE: INFERENCE\n"
     ]
    }
   ],
   "source": [
    "import src.generator.QAM as QAM\n",
    "\n",
    "# MODE = \"TRAINING\"\n",
    "MODE = \"INFERENCE\"\n",
    "print(f\"MODE: {MODE}\")\n",
    "\n",
    "DATASET_SOURCE = \"394\" # 实验数据序号 或者 SYNTH\n",
    "# DATASET_SOURCE = \"SYNTH\"\n",
    "\n",
    "SEED = 1\n",
    "CONSTELLATION = \"QAM\"\n",
    "QAM_ORDER = 64\n",
    "NUMBEROFSYMBOLS = 51200\n",
    "TAPS = 35\n",
    "\n",
    "cons = QAM.read_constellation_file(QAM_ORDER, CONSTELLATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from tqdm import notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(60, 300): 357, (60, 350): 359, (60, 400): 361, (60, 450): 363, (60, 500): 367, (60, 550): 399, (60, 600): 401, (70, 300): 352, (70, 350): 350, (70, 400): 343, (70, 450): 345, (70, 500): 347, (70, 550): 355, (70, 600): 397, (80, 450): 326, (80, 500): 329, (80, 550): 332, (80, 600): 334, (80, 650): 336, (80, 700): 339, (80, 750): 341, (90, 300): 394, (90, 350): 386, (90, 400): 384, (90, 450): 309, (90, 500): 311, (90, 550): 314, (90, 600): 316, (90, 650): 318, (90, 700): 321, (90, 750): 323, (100, 300): 369, (100, 350): 371, (100, 400): 373, (100, 450): 375, (100, 500): 377, (100, 550): 379, (100, 600): 381}\n",
      "(60, 300): [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "(60, 350): [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "(60, 400): [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "(60, 450): [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "(60, 500): [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "(60, 550): [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "(60, 600): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "(70, 300): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "(70, 350): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "(70, 400): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "(70, 450): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "(70, 500): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "(70, 550): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "(70, 600): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "(80, 450): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "(80, 500): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "(80, 550): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "(80, 600): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "(80, 650): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "(80, 700): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "(80, 750): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "(90, 300): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "(90, 350): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "(90, 400): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "(90, 450): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "(90, 500): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "(90, 550): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "(90, 600): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "(90, 650): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "(90, 700): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "(90, 750): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "(100, 300): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "(100, 350): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "(100, 400): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "(100, 450): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "(100, 500): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "(100, 550): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "(100, 600): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Data processing\n",
    "\n",
    "import json\n",
    "\n",
    "# Input JSON data\n",
    "json_data = \"\"\"\n",
    "{\n",
    "  \"data\": [\n",
    "    {\n",
    "      \"ID\": 357,\n",
    "      \"I\": 60,\n",
    "      \"VPP\": 300,\n",
    "      \"best\": 0.042237062\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 359,\n",
    "      \"I\": 60,\n",
    "      \"VPP\": 350,\n",
    "      \"best\": 0.0047104652\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 361,\n",
    "      \"I\": 60,\n",
    "      \"VPP\": 400,\n",
    "      \"best\": 0.0076074503\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 363,\n",
    "      \"I\": 60,\n",
    "      \"VPP\": 450,\n",
    "      \"best\": 0.009638712\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 367,\n",
    "      \"I\": 60,\n",
    "      \"VPP\": 500,\n",
    "      \"best\": 0.0052374873\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 399,\n",
    "      \"I\": 60,\n",
    "      \"VPP\": 550,\n",
    "      \"best\": 0.024265933\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 401,\n",
    "      \"I\": 60,\n",
    "      \"VPP\": 600,\n",
    "      \"best\": 0.039526662\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 352,\n",
    "      \"I\": 70,\n",
    "      \"VPP\": 300,\n",
    "      \"best\": 0.0042554584\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 350,\n",
    "      \"I\": 70,\n",
    "      \"VPP\": 350,\n",
    "      \"best\": 0.0033094373\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 343,\n",
    "      \"I\": 70,\n",
    "      \"VPP\": 400,\n",
    "      \"best\": 0.0021048152\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 345,\n",
    "      \"I\": 70,\n",
    "      \"VPP\": 450,\n",
    "      \"best\": 0.0028151494\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 347,\n",
    "      \"I\": 70,\n",
    "      \"VPP\": 500,\n",
    "      \"best\": 0.0044584111\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 355,\n",
    "      \"I\": 70,\n",
    "      \"VPP\": 550,\n",
    "      \"best\": 0.00954204722\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 397,\n",
    "      \"I\": 70,\n",
    "      \"VPP\": 600,\n",
    "      \"best\": 0.012746735\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 326,\n",
    "      \"I\": 80,\n",
    "      \"VPP\": 450,\n",
    "      \"best\": 0.0022881273\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 329,\n",
    "      \"I\": 80,\n",
    "      \"VPP\": 500,\n",
    "      \"best\": 0.0026318374\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 332,\n",
    "      \"I\": 80,\n",
    "      \"VPP\": 550,\n",
    "      \"best\": 0.0030213755\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 334,\n",
    "      \"I\": 80,\n",
    "      \"VPP\": 600,\n",
    "      \"best\": 0.0051458313\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 336,\n",
    "      \"I\": 80,\n",
    "      \"VPP\": 650,\n",
    "      \"best\": 0.0073815837\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 339,\n",
    "      \"I\": 80,\n",
    "      \"VPP\": 700,\n",
    "      \"best\": 0.010877606\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 341,\n",
    "      \"I\": 80,\n",
    "      \"VPP\": 750,\n",
    "      \"best\": 0.017869652\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 394,\n",
    "      \"I\": 90,\n",
    "      \"VPP\": 300,\n",
    "      \"best\": 0.017869652\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 386,\n",
    "      \"I\": 90,\n",
    "      \"VPP\": 350,\n",
    "      \"best\": 0.0062129693\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 384,\n",
    "      \"I\": 90,\n",
    "      \"VPP\": 400,\n",
    "      \"best\": 0.0060983993\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 309,\n",
    "      \"I\": 90,\n",
    "      \"VPP\": 450,\n",
    "      \"best\": 0.0028020557\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 311,\n",
    "      \"I\": 90,\n",
    "      \"VPP\": 500,\n",
    "      \"best\": 0.0051752922\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 314,\n",
    "      \"I\": 90,\n",
    "      \"VPP\": 550,\n",
    "      \"best\": 0.0040164981\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 316,\n",
    "      \"I\": 90,\n",
    "      \"VPP\": 600,\n",
    "      \"best\": 0.004579528\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 318,\n",
    "      \"I\": 90,\n",
    "      \"VPP\": 650,\n",
    "      \"best\": 0.0060558447\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 321,\n",
    "      \"I\": 90,\n",
    "      \"VPP\": 700,\n",
    "      \"best\": 0.0063700939\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 323,\n",
    "      \"I\": 90,\n",
    "      \"VPP\": 750,\n",
    "      \"best\": 0.012262267\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 369,\n",
    "      \"I\": 100,\n",
    "      \"VPP\": 300,\n",
    "      \"best\": 0.01826901\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 371,\n",
    "      \"I\": 100,\n",
    "      \"VPP\": 350,\n",
    "      \"best\": 0.0095387738\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 373,\n",
    "      \"I\": 100,\n",
    "      \"VPP\": 400,\n",
    "      \"best\": 0.0071524436\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 375,\n",
    "      \"I\": 100,\n",
    "      \"VPP\": 450,\n",
    "      \"best\": 0.0056663066\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 377,\n",
    "      \"I\": 100,\n",
    "      \"VPP\": 500,\n",
    "      \"best\": 0.0070280533\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 379,\n",
    "      \"I\": 100,\n",
    "      \"VPP\": 550,\n",
    "      \"best\": 0.0069625847\n",
    "    },\n",
    "    {\n",
    "      \"ID\": 381,\n",
    "      \"I\": 100,\n",
    "      \"VPP\": 600,\n",
    "      \"best\": 0.0069822253\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Load the JSON string into a Python dictionary\n",
    "data = json.loads(json_data)\n",
    "\n",
    "# Initialize an empty dictionary to store the mapping\n",
    "iv_to_id_map = {}\n",
    "\n",
    "# Process each entry in the \"data\" list\n",
    "for entry in data['data']:\n",
    "    i = entry['I']\n",
    "    vpp = entry['VPP']\n",
    "    id_val = entry['ID']\n",
    "    iv_to_id_map[(i, vpp)] = id_val\n",
    "\n",
    "# Output the organized dictionary\n",
    "print(iv_to_id_map)\n",
    "\n",
    "# Collect all unique (I, VPP) combinations\n",
    "unique_combinations = sorted(set((entry['I'], entry['VPP']) for entry in data['data']))\n",
    "\n",
    "# Create a mapping from combination to index\n",
    "combination_to_index = {combo: idx for idx, combo in enumerate(unique_combinations)}\n",
    "\n",
    "# Generate one-hot-encoded vectors for each entry in the data\n",
    "one_hot_vectors = {}\n",
    "for entry in data['data']:\n",
    "    combo = (entry['I'], entry['VPP'])\n",
    "    vector = np.zeros(len(unique_combinations))\n",
    "    vector[combination_to_index[combo]] = 1\n",
    "    one_hot_vectors[combo] = vector\n",
    "\n",
    "# Output the one-hot vectors for verification\n",
    "for combo, vector in one_hot_vectors.items():\n",
    "    print(f\"{combo}: {vector.tolist()}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OSC_sync_357.txt\n",
      "[(60, 300)]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 1 is not equal to len(dims) = 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 123\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28mprint\u001b[39m(unique_labels)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# Create the dataset\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m dataset \u001b[38;5;241m=\u001b[39m CustomDataset(\n\u001b[0;32m    124\u001b[0m     folder_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    125\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[0;32m    126\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m    127\u001b[0m     label_mapping\u001b[38;5;241m=\u001b[39mlabel_mapping,\n\u001b[0;32m    128\u001b[0m     classes\u001b[38;5;241m=\u001b[39munique_labels,\n\u001b[0;32m    129\u001b[0m     num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m\n\u001b[0;32m    130\u001b[0m )\n\u001b[0;32m    132\u001b[0m TRAIN_RATIO \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.6\u001b[39m\n\u001b[0;32m    133\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m\n",
      "Cell \u001b[1;32mIn[13], line 55\u001b[0m, in \u001b[0;36mCustomDataset.__init__\u001b[1;34m(self, folder_path, data_files, labels, label_mapping, classes, num_samples)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)  \u001b[38;5;66;03m# Use long for class indices\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Rearrange dimensions to (features, sequence_length) ➔ (2, 512)\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# print(self.dataset.shape)\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Swaps the last two dimensions\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Store the number of unique classes\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(label_mapping)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 1 is not equal to len(dims) = 2"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple, Any\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from os import path\n",
    "\n",
    "# Local\n",
    "from src.data.processing import parse_str\n",
    "from src.data.file_handler import read_file\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            folder_path: str = None,\n",
    "            data_files:  List[str] = None,\n",
    "            labels:      List[Any] = None,\n",
    "            label_mapping: dict[Any, int] = None,\n",
    "            classes=None,\n",
    "            num_samples: int = 512,\n",
    "          ):\n",
    "\n",
    "        if not folder_path or not data_files or not labels or label_mapping is None:\n",
    "            self.dataset = None\n",
    "            self.labels = None\n",
    "            self.size = 0\n",
    "            self.num_classes = 0\n",
    "            return\n",
    "\n",
    "        all_datasets = []\n",
    "        all_labels = []\n",
    "\n",
    "        for data_file, label in zip(data_files, labels):\n",
    "            data_path = path.join(folder_path, data_file)\n",
    "            dataset = parse_str(read_file(data_path))\n",
    "            sequences, seq_labels = self.split_sequence(\n",
    "                dataset,\n",
    "                label_mapping[label],  # Encode label as integer index\n",
    "                num_samples\n",
    "            )\n",
    "            all_datasets.append(sequences)\n",
    "            all_labels.append(seq_labels)\n",
    "\n",
    "        # Concatenate all datasets and labels\n",
    "        self.dataset = np.concatenate(all_datasets, axis=0)\n",
    "        self.labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "        self.classes = classes\n",
    "\n",
    "        # Convert to tensor\n",
    "        self.dataset = torch.tensor(self.dataset, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.long)  # Use long for class indices\n",
    "        # Rearrange dimensions to (features, sequence_length) ➔ (2, 512)\n",
    "        # print(self.dataset.shape)\n",
    "\n",
    "        self.dataset = self.dataset.squeeze().permute(1, 0)  # Swaps the last two dimensions\n",
    "        # Store the number of unique classes\n",
    "        self.num_classes = len(label_mapping)\n",
    "\n",
    "        print(f\"Number of classes: {self.num_classes}\")\n",
    "        print(\"Sample data:\", self.dataset[:2])\n",
    "        print(\"Sample labels:\", self.labels[:2])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx], self.labels[idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def split_sequence(\n",
    "            dataset: List[List[float]], label: int, win_len: int\n",
    "        ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Splits the dataset into sequences of length `win_len` and assigns the provided label.\n",
    "\n",
    "        Args:\n",
    "            dataset (List[List[float]]): The input dataset as a list of lists.\n",
    "            label (int): The integer label to assign to each sequence.\n",
    "            win_len (int): The length of the sliding window.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[np.ndarray, np.ndarray]: A tuple containing:\n",
    "                - `new_dataset` (np.ndarray): Sequences split from the dataset.\n",
    "                - `new_labels` (np.ndarray): Labels for each sequence.\n",
    "        \"\"\"\n",
    "        if len(dataset) < win_len:\n",
    "            raise ValueError(\"Dataset length must be greater than or equal to the window length.\")\n",
    "\n",
    "        new_dataset = []\n",
    "        new_labels = []\n",
    "\n",
    "        # Sliding window implementation\n",
    "        for j in range(len(dataset) - win_len + 1):\n",
    "            start_index = j\n",
    "            end_index = start_index + win_len\n",
    "            new_dataset.append(dataset[start_index:end_index])\n",
    "            new_labels.append(label)\n",
    "\n",
    "        return np.array(new_dataset), np.array(new_labels)\n",
    "\n",
    "# Prepare the list of data files and their corresponding labels\n",
    "data_files = []\n",
    "labels = []\n",
    "\n",
    "i = 0\n",
    "for (label, set_num) in iv_to_id_map.items():\n",
    "    data_file = f\"OSC_sync_{set_num}.txt\"  # Assuming file names are based on set_num\n",
    "    print(data_file)\n",
    "    data_files.append(data_file)\n",
    "    labels.append(label)  # Labels are tuples like (I, VPP)\n",
    "    i = i + 1\n",
    "    if i == 1:\n",
    "        break\n",
    "\n",
    "# Create a mapping from labels to integer indices\n",
    "unique_labels = sorted(set(labels))\n",
    "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "print(unique_labels)\n",
    "\n",
    "# Create the dataset\n",
    "dataset = CustomDataset(\n",
    "    folder_path=\"./dataset\",\n",
    "    data_files=data_files,\n",
    "    labels=labels,\n",
    "    label_mapping=label_mapping,\n",
    "    classes=unique_labels,\n",
    "    num_samples=512\n",
    ")\n",
    "\n",
    "TRAIN_RATIO = 0.6\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "class CustomSubset(Subset):\n",
    "    def __init__(self, dataset, indices):\n",
    "        super().__init__(dataset, indices)\n",
    "        # Copy custom attributes\n",
    "        self.num_classes = dataset.num_classes\n",
    "        self.classes = dataset.classes\n",
    "        # Add any other custom variables you need\n",
    "\n",
    "# Modify your splitting code\n",
    "def custom_random_split(dataset, lengths):\n",
    "    \"\"\"\n",
    "    Splits the dataset into non-overlapping new datasets of given lengths.\n",
    "    Returns a list of CustomSubset instances.\n",
    "    \"\"\"\n",
    "    from torch.utils.data import random_split\n",
    "    subsets = random_split(dataset, lengths)\n",
    "    return [CustomSubset(subset.dataset, subset.indices) for subset in subsets]\n",
    "\n",
    "train_size = int(TRAIN_RATIO * len(dataset))\n",
    "print(f\"Traning ratio (train:valid): {TRAIN_RATIO}:{1 - TRAIN_RATIO}\")\n",
    "print(f\"Training size: {train_size} | Validation size: {len(dataset) - train_size}\")\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = custom_random_split(\n",
    "    dataset, [train_size, val_size]\n",
    ")\n",
    "print(type(train_dataset))\n",
    "# Setup the training and validation data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nn.TRANSFORMER import Hybrid\n",
    "\n",
    "model = Hybrid(input_samples=512, n_classes=num_classes,debug=False)\n",
    "model.to(device)\n",
    "model = nn.DataParallel(model, device_ids=[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conf_matrix = np.zeros((len(train_dataset.classes),len(train_dataset.classes)))\n",
    "conf_matrix_2 = np.zeros((len(val_dataset.classes),len(val_dataset.classes)))\n",
    "epoch_nums = 16\n",
    "best_accuracy = -1000\n",
    "\n",
    "criterion_classifier = nn.CrossEntropyLoss()\n",
    "criterion_recon = nn.MSELoss()\n",
    "optimizer =  torch.optim.Adam(model.parameters(), lr=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d677f669285b4090a6a5abb084905c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b8ec01aa2848f49febda56e1a22d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 512, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [128, 2, 1], expected input[512, 512, 1] to have 2 channels, but got 512 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(RXinputs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(RXinputs)\n\u001b[0;32m     19\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     21\u001b[0m loss_classify \u001b[38;5;241m=\u001b[39m alpha\u001b[38;5;241m*\u001b[39mcriterion_classifier(outputs,labels)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\parallel\\data_parallel.py:191\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    188\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m ({},)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    192\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[: \u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m    193\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, module_kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\dsp-project\\src\\nn\\TRANSFORMER.py:65\u001b[0m, in \u001b[0;36mHybrid.forward\u001b[1;34m(self, input_)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_):\n\u001b[1;32m---> 65\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(input_)\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug: \u001b[38;5;28mprint\u001b[39m(z\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     69\u001b[0m     recon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(z)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\dsp-project\\src\\nn\\TRANSFORMER.py:12\u001b[0m, in \u001b[0;36mConvBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 12\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[0;32m     13\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(out)\n\u001b[0;32m     14\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\conv.py:375\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\conv.py:370\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[0;32m    360\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    361\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    369\u001b[0m     )\n\u001b[1;32m--> 370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[0;32m    371\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[0;32m    372\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [128, 2, 1], expected input[512, 512, 1] to have 2 channels, but got 512 channels instead"
     ]
    }
   ],
   "source": [
    "\n",
    "alpha = 1\n",
    "beta = 1\n",
    "\n",
    "for epoch in notebook.tqdm(range(epoch_nums),desc='Epoch'):\n",
    "    model.train()\n",
    "    for item in notebook.tqdm(train_loader,desc='Training',leave=False):\n",
    "        # print(item[1])\n",
    "        # RXinputs = item['data'].to(device)\n",
    "        # TXinputs = item['data_Tx'].to(device)\n",
    "        # labels = item['label'].to(device)\n",
    "        RXinputs = item[0].to(device)\n",
    "        labels = item[1].to(device)\n",
    "\n",
    "        print(RXinputs.shape)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(RXinputs)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        loss_classify = alpha*criterion_classifier(outputs,labels)\n",
    "        # loss_recon = beta*(criterion_recon(recon,TXinputs))\n",
    "        # loss = loss_classify + loss_recon\n",
    "        loss = loss_classify\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch == epoch_nums-1:\n",
    "            for i in range(RXinputs.shape[0]):\n",
    "                label = labels[i]\n",
    "                pred = predicted[i]\n",
    "                conf_matrix[label,pred]+=1\n",
    "        \n",
    "    correct = 0    \n",
    "    model.eval()    \n",
    "    for item in notebook.tqdm(val_loader,desc='Validation',leave=False):\n",
    "        RXinputs = item['data'].to(device)\n",
    "        TXinputs = item['data_Tx'].to(device)\n",
    "        labels = item['label'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs,recon = model(RXinputs)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == labels).sum().cpu().data.numpy()\n",
    "        \n",
    "        if epoch == epoch_nums-1:\n",
    "            for i in range(RXinputs.shape[0]):\n",
    "                label = labels[i]\n",
    "                pred = predicted[i]\n",
    "                conf_matrix_2[label,pred]+=1\n",
    "        \n",
    "    accuracy = correct/float(len(val_loader)*BATCH_SIZE)\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        #torch.save(model.state_dict(), \"Trained_Models/classification.pt\")    \n",
    "\n",
    "    if epoch == epoch_nums-1: \n",
    "        for i in range(len(conf_matrix_2)):\n",
    "            conf_matrix_2[i]=conf_matrix_2[i]/conf_matrix_2[i].sum() \n",
    "            \n",
    "        for i in range(len(conf_matrix)):\n",
    "            conf_matrix[i]=conf_matrix[i]/conf_matrix[i].sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
